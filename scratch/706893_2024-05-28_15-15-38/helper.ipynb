{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get first and last trial range (based on trained video) to plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "arhmm_latents_path = '/local1/video-analysis/scratch/706893_2024-05-28_15-15-38/results/xinxin/coupled-baiting_' + bodypart + '/706893/2024-05-28/dfs/arhmm_latents_df_.csv'\n",
    "df_original_path = '/local1/video-analysis/scratch/706893_2024-05-28_15-15-38/results/xinxin/coupled-baiting_' + bodypart + '/706893/2024-05-28/dfs/df_original.csv'\n",
    "\n",
    "# Load the CSV files\n",
    "arhmm_latents_df = pd.read_csv(arhmm_latents_path)\n",
    "df_original = pd.read_csv(df_original_path)\n",
    "\n",
    "# Get the first Harp_Timestamp from arhmm_latents_df\n",
    "first_harp_timestamp = arhmm_latents_df[\"Harp_Timestamp\"].iloc[0]\n",
    "\n",
    "# Find the closest next trial timestamp in df_original\n",
    "first_analyzed_trial_idx = (df_original[\"start_time\"] > first_harp_timestamp).idxmax()  # Get index of the first trial after first_harp_timestamp in analyzed video\n",
    "first_analyzed_trial_event_timestamp = df_original.loc[first_analyzed_trial_idx, \"start_time\"]\n",
    "first_analyzed_trial_number = df_original.loc[first_analyzed_trial_idx, \"Trial\"]\n",
    "\n",
    "# Print results\n",
    "print(f\"First Harp_Timestamp from arhmm_latents_df: {first_harp_timestamp}\")\n",
    "print(f\"Closest next trial Harp_Timestamp (start_time) from df_original: {first_analyzed_trial_event_timestamp} (Trial {first_analyzed_trial_number})\")\n",
    "print('')\n",
    "\n",
    "# Get the last non-NaN Harp_Timestamp from arhmm_latents_df\n",
    "last_harp_timestamp = arhmm_latents_df[\"Harp_Timestamp\"].dropna().iloc[-1]\n",
    "\n",
    "# # Find the closest prior trial timestamp in df_original\n",
    "# Drop NaNs from the Harp_Timestamp column\n",
    "df_original_clean = df_original.dropna(subset=[\"stop_time\"])\n",
    "\n",
    "# Find the index of the last trial before last_harp_timestamp\n",
    "last_analyzed_trial_idx = df_original_clean[df_original_clean[\"stop_time\"] < last_harp_timestamp].index[-1]\n",
    "last_analyzed_trial_event_timestamp = df_original.loc[last_analyzed_trial_idx, \"stop_time\"]\n",
    "\n",
    "# Get the trial number of the closest prior trial\n",
    "last_analyzed_trial_number = df_original_clean.loc[last_analyzed_trial_idx, \"Trial\"]\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"Last Harp_Timestamp from arhmm_latents_df: {last_harp_timestamp}\")\n",
    "print(f\"Closest prior trial Harp_Timestamp from df_events: {last_analyzed_trial_event_timestamp} (Trial {last_analyzed_trial_number})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "First Harp_Timestamp from arhmm_latents_df: 6574786.711488\n",
    "Closest next trial Harp_Timestamp (start_time) from df_original: 6574787.602496 (Trial 9)\n",
    "\n",
    "Last Harp_Timestamp from arhmm_latents_df: 6578185.45952\n",
    "Closest prior trial Harp_Timestamp from df_events: 6578181.317504 (Trial 273)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get num_initial_frames_to_exclude and num_late_frames_to_exclude "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example usage\n",
    "hdf5_path = '/local1/video-analysis/scratch/706893_2024-05-28_15-15-38/data/xinxin/coupled-baiting_' + bodypart + '/706893/2024-05-28/data.hdf5'\n",
    "set_harp_ts = True  \n",
    "\n",
    "ae_latent_path = '/local1/video-analysis/scratch/706893_2024-05-28_15-15-38/results/xinxin/coupled-baiting_' + bodypart + '/706893/2024-05-28/dfs/ae_latents_df.csv'\n",
    "ae_latents_df = pd.read_csv(ae_latent_path)\n",
    "\n",
    "# Initialize exclusion counters\n",
    "total_num_initial_frames_to_exclude, total_num_late_frames_to_exclude = 0, 0\n",
    "\n",
    "if set_harp_ts:\n",
    "    # Exclude frames from the beginning of the list\n",
    "    selected_segments = np.arange(ae_latents_df['seg_index'].iloc[0])  \n",
    "\n",
    "    n_frames_dict, total_num_initial_frames_to_exclude, total_num_frames_in_segments = get_frame_counts(hdf5_path, selected_segments)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Total number of segments in the HDF5 file:\", total_num_frames_in_segments)\n",
    "    print(\"Total number of excluded frames for EARLY segments:\", total_num_initial_frames_to_exclude)\n",
    "    print(\"Frame counts per segments:\", n_frames_dict)\n",
    "\n",
    "\n",
    "    # Exclude frames from the end of the list\n",
    "    selected_segments = np.arange(ae_latents_df['seg_index'].iloc[-1]+1, total_num_frames_in_segments)  \n",
    "\n",
    "    n_frames_dict_, total_num_late_frames_to_exclude, _ = get_frame_counts(hdf5_path, selected_segments)\n",
    "\n",
    "    # Print results\n",
    "    print('')\n",
    "    print(\"Total number of excluded frames for LATE segments:\", total_num_late_frames_to_exclude)\n",
    "    print(\"Frame counts per segments:\", n_frames_dict_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total number of segments in the HDF5 file: 178\n",
    "Total number of excluded frames for EARLY segments: 4071\n",
    "Frame counts per segments: {'trial_0000': 1000, 'trial_0001': 1071, 'trial_0002': 1000, 'trial_0003': 1000}\n",
    "\n",
    "Total number of excluded frames for LATE segments: 3667\n",
    "Frame counts per segments: {'trial_0174': 994, 'trial_0175': 953, 'trial_0176': 1000, 'trial_0177': 720}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the number of frames in the selected_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def get_frame_counts(hdf5_path, selected_trials=None):\n",
    "    \"\"\"\n",
    "    Reads an HDF5 file and extracts the number of frames for each selected trial.\n",
    "\n",
    "    Parameters:\n",
    "        hdf5_path (str): Path to the HDF5 file.\n",
    "        selected_trials (list, optional): List of trial indices to include. If None, all trials will be used.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with trial keys and their corresponding frame counts.\n",
    "        int: The total number of frames across the selected trials.\n",
    "        int: The total number of trials in the HDF5 file.\n",
    "    \"\"\"\n",
    "    n_frames_dict = {}\n",
    "\n",
    "    with h5py.File(hdf5_path, 'r') as f:\n",
    "        images_group = f['images']\n",
    "        total_num_trials = len(images_group.keys())  # Get the total number of trials\n",
    "\n",
    "        # If no specific trials are selected, use all available trials\n",
    "        if selected_trials is None:\n",
    "            selected_trials = list(range(total_num_trials))\n",
    "\n",
    "        # Loop over the selected trial indices\n",
    "        for idx in selected_trials:\n",
    "            trial_key = f\"trial_{idx:04d}\"\n",
    "            if trial_key in images_group:\n",
    "                n_frames = images_group[trial_key].shape[0]  # Extract number of frames\n",
    "                n_frames_dict[trial_key] = n_frames\n",
    "\n",
    "    # Compute total frames across selected trials\n",
    "    total_num_frames = sum(n_frames_dict.values())\n",
    "\n",
    "    return n_frames_dict, total_num_frames, total_num_trials\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_frames_in_segments = 178\n",
    "total_num_initial_frames_to_exclude = 4071\n",
    "total_num_late_frames_to_exclude = 3667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_harp_timestamp = 6574786.711488\n",
    "first_analyzed_trial_event_timestamp = 6574787.602496\n",
    "first_analyzed_trial_number = 9\n",
    "\n",
    "last_harp_timestamp = 6578185.480352\n",
    "last_analyzed_trial_event_timestamp = 6578181.317504\n",
    "last_analyzed_trial_number = 273"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def get_frame_counts(hdf5_path, selected_trials=None):\n",
    "    \"\"\"\n",
    "    Reads an HDF5 file and extracts the number of frames for each selected trial.\n",
    "\n",
    "    Parameters:\n",
    "        hdf5_path (str): Path to the HDF5 file.\n",
    "        selected_trials (list, optional): List of trial indices to include. If None, all trials will be used.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with trial keys and their corresponding frame counts.\n",
    "        int: The total number of frames across the selected trials.\n",
    "        int: The total number of trials in the HDF5 file.\n",
    "    \"\"\"\n",
    "    n_frames_dict = {}\n",
    "\n",
    "    with h5py.File(hdf5_path, 'r') as f:\n",
    "        images_group = f['images']\n",
    "        total_num_trials = len(images_group.keys())  # Get the total number of trials\n",
    "\n",
    "        # If no specific trials are selected, use all available trials\n",
    "        if selected_trials is None:\n",
    "            selected_trials = list(range(total_num_trials))\n",
    "\n",
    "        # Loop over the selected trial indices\n",
    "        for idx in selected_trials:\n",
    "            trial_key = f\"trial_{idx:04d}\"\n",
    "            if trial_key in images_group:\n",
    "                n_frames = images_group[trial_key].shape[0]  # Extract number of frames\n",
    "                n_frames_dict[trial_key] = n_frames\n",
    "\n",
    "    # Compute total frames across selected trials\n",
    "    total_num_frames = sum(n_frames_dict.values())\n",
    "\n",
    "    return n_frames_dict, total_num_frames, total_num_trials\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ae_latents_str = '08' #16\n",
    "bodypart = 'ear'  # set body part accordingly, like 'nose', 'tongue_mouth'\n",
    "fps_label = ''# '_480Hz'\n",
    "bodypart = f\"{bodypart}{fps_label}\"\n",
    "\n",
    "\n",
    "n_arhmm_states = [2,4,8,12,16,20,32]  \n",
    "original_video_fps = 480\n",
    "\n",
    "\n",
    "import os\n",
    "# Define the directory path\n",
    "dfs_dir = f\"/local1/video-analysis/scratch/706893_2024-05-28_15-15-38/results/xinxin/coupled-baiting_{bodypart}/706893/2024-05-28/dfs\"\n",
    "# Ensure the directory exists\n",
    "os.makedirs(dfs_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "nwb_dir = \"/local1/video-analysis/scratch/706893_2024-05-28_15-15-38/nwb\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from behavenet import get_user_dir, make_dir_if_not_exists\n",
    "from behavenet.fitting.utils import get_expt_dir\n",
    "from behavenet.fitting.utils import get_session_dir\n",
    "from behavenet.fitting.utils import get_best_model_version\n",
    "from behavenet.fitting.utils import get_lab_example\n",
    "\n",
    "save_outputs = True  # true to save figures/movies to user's figure directory\n",
    "format = 'png'  # figure format ('png' | 'jpeg' | 'pdf'); movies saved as mp4\n",
    "\n",
    "\n",
    "\n",
    "# read the valuse of hparams from meta_tag.csv\n",
    "import pandas as pd\n",
    "\n",
    "# set the path to the meta_tag.csv file\n",
    "\n",
    "# Path to your CSV file\n",
    "# file_path = \"meta_tag.csv\"\n",
    "meta_tag_file_path = '/local1/video-analysis/scratch/706893_2024-05-28_15-15-38/results/xinxin/coupled-baiting_' +bodypart+ '/706893/2024-05-28/ae/conv/' +n_ae_latents_str+ '_latents/' +bodypart+ '_3_views/version_0/meta_tags.csv'\n",
    "\n",
    "# Load CSV\n",
    "df_ = pd.read_csv(meta_tag_file_path)\n",
    "\n",
    "# Extract frame rate\n",
    "ae_model_experiment_name = df_[df_[\"key\"] == \"experiment_name\"][\"value\"].values[0]\n",
    "n_ae_latents = int(df_[df_[\"key\"] == \"n_ae_latents\"][\"value\"].values[0])\n",
    "experimenter_name = df_[df_[\"key\"] == \"lab\"][\"value\"].values[0]\n",
    "dataset_hparams_experiment_name = df_[df_[\"key\"] == \"expt\"][\"value\"].values[0]\n",
    "frame_rate = float(df_[df_[\"key\"] == \"frame_rate\"][\"value\"].values[0])\n",
    "ae_model_version = df_[df_[\"key\"] == \"version\"][\"value\"].values[0]\n",
    "\n",
    "print(\"ae_model_experiment_name:\", ae_model_experiment_name)\n",
    "print(\"n_ae_latents:\", n_ae_latents)\n",
    "print(\"experimenter_name:\", experimenter_name)\n",
    "print(\"dataset_hparams_experiment_name:\", dataset_hparams_experiment_name)\n",
    "print(\"frame_rate:\", frame_rate)\n",
    "print(\"ae_model_version:\", ae_model_version)\n",
    "\n",
    "# total_num_trials = total_num_trials # calculated before \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Extract the version number\n",
    "version = meta_tag_file_path.split(\"version_\")[1].split(\"/\")[0]\n",
    "\n",
    "print(\"version assigned in meta_tag_file_path path: \", version) \n",
    "print(\"'version' as stored in meta_tag.csv: \", ae_model_version)\n",
    "\n",
    "assert version == ae_model_version, \"Version mismatch\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Define the updated file path\n",
    "file_path = '/local1/video-analysis/scratch/706893_2024-05-28_15-15-38/results/xinxin/coupled-baiting_' +bodypart+ '/706893/2024-05-28/ae/conv/' +n_ae_latents_str+ '_latents/' +bodypart+ '_3_views/version_0/xinxin_coupled-baiting_' +bodypart+ '_706893_2024-05-28_latents.pkl'\n",
    "\n",
    "# Load the updated pickle file\n",
    "with open(file_path, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Extract latents and trials from the updated file\n",
    "ae_latents = data.get(\"latents\", [])\n",
    "ae_trials = data.get(\"trials\", {})\n",
    "\n",
    "# Convert latents to a structured DataFrame\n",
    "ae_latent_trial_df = []\n",
    "for i, arr in enumerate(ae_latents):\n",
    "    if arr.size > 0:  # Ignore empty arrays\n",
    "        df_temp = pd.DataFrame(arr)\n",
    "        df_temp['seg_index'] = i  # Identify the latent group\n",
    "        ae_latent_trial_df.append(df_temp)\n",
    "\n",
    "proto_ae_latents_df = pd.concat(ae_latent_trial_df, ignore_index=True) if ae_latent_trial_df else pd.DataFrame()\n",
    "# save proto_ae_latents_df to csv in dfs folder\n",
    "proto_ae_latents_df.to_csv(f'/local1/video-analysis/scratch/706893_2024-05-28_15-15-38/results/xinxin/coupled-baiting_{bodypart}/706893/2024-05-28/dfs/proto_ae_latents_df.csv', index=False)\n",
    "\n",
    "\n",
    "# Convert trials into a structured DataFrame\n",
    "trial_type_df = pd.DataFrame([(k, list(v)) for k, v in ae_trials.items()], columns=[\"Trial Type\", \"Indices\"])\n",
    "\n",
    "# Display the first few rows of both DataFrames\n",
    "print(\"=== Latents Data ===\")\n",
    "print(trial_type_df[1000:1300])  # Show first 10 rows\n",
    "\n",
    "print(\"\\n=== Trials Data ===\")\n",
    "print(trial_type_df)  # Show structured trial indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_indices = ae_trials.get(\"train\", [])\n",
    "test_indices = ae_trials.get(\"test\", [])\n",
    "val_indices = ae_trials.get(\"val\", [])\n",
    "\n",
    "\n",
    "\n",
    "print(len(train_indices), len(test_indices), len(val_indices))\n",
    "print('train_indices:', train_indices)\n",
    "print('test_indices:', test_indices)\n",
    "print('val_indices:', val_indices)\n",
    "# buffer makes sense now considering the way they assign data to data dtypes\n",
    "\n",
    "\n",
    "\n",
    "# Combine all indices into one array\n",
    "all_indices = np.concatenate((train_indices, test_indices, val_indices))\n",
    "\n",
    "# Find the maximum value among all indices\n",
    "max_value = np.max(all_indices)\n",
    "\n",
    "# Create a full set of numbers from 0 to max_value (inclusive)\n",
    "full_range = np.arange(max_value)\n",
    "\n",
    "# Find missing numbers by computing the set difference\n",
    "excluded_trials = np.setdiff1d(full_range, all_indices)\n",
    "\n",
    "# print(\"Total num segmentss is \", total_num_trials)\n",
    "print(\"Max trial index present is \", max_value)\n",
    "print(\"Missing numbers from 0 to\", max_value-1, \"are:\", excluded_trials)\n",
    "\n",
    "# ???? has problem: excluded trial from the tail are not here\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "train_indices = ae_trials.get(\"train\", [])\n",
    "test_indices = ae_trials.get(\"test\", [])\n",
    "val_indices = ae_trials.get(\"val\", [])\n",
    "\n",
    "# Combine all indices into one array and sort them\n",
    "all_indices = np.concatenate((train_indices, test_indices, val_indices))\n",
    "all_indices = np.sort(all_indices)\n",
    "\n",
    "# get the max and min segment in the data\n",
    "max_seg_idx = np.max(all_indices)\n",
    "min_seg_idx = np.min(all_indices)\n",
    "\n",
    "print(\"Max segment index present is \", max_seg_idx)\n",
    "print(\"Min segment present is \", min_seg_idx)\n",
    "\n",
    "# Find missing numbers by computing the set difference\n",
    "excluded_trials = np.setdiff1d(np.arange(max_seg_idx), all_indices)\n",
    "print(\"Missing numbers from 0 to\", max_seg_idx, \"segment are:\", excluded_trials)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example usage\n",
    "hdf5_path = '/local1/video-analysis/scratch/706893_2024-05-28_15-15-38/data/xinxin/coupled-baiting_' + bodypart + '/706893/2024-05-28/data.hdf5'\n",
    "# set_harp_ts = True\n",
    "# Initialize exclusion counters\n",
    "total_num_initial_frames_to_exclude, total_num_late_frames_to_exclude = 0, 0\n",
    "\n",
    "# Exclude frames from the beginning of the list\n",
    "selected_segments = np.arange(min_seg_idx)\n",
    "\n",
    "n_frames_dict, total_num_initial_frames_to_exclude, total_num_frames_in_segments = get_frame_counts(hdf5_path, selected_segments)\n",
    "\n",
    "# Print results\n",
    "print(\"Total number of segments in the HDF5 file:\", total_num_frames_in_segments)\n",
    "print(\"Total number of excluded frames for EARLY segments:\", total_num_initial_frames_to_exclude)\n",
    "print(\"Frame counts per segments:\", n_frames_dict)\n",
    "\n",
    "\n",
    "# Exclude frames from the end of the list\n",
    "selected_segments = np.arange(max_seg_idx+1, total_num_frames_in_segments)  \n",
    "\n",
    "n_frames_dict_, total_num_late_frames_to_exclude, _ = get_frame_counts(hdf5_path, selected_segments)\n",
    "\n",
    "# Print results\n",
    "print('')\n",
    "print(\"Total number of excluded frames for LATE segments:\", total_num_late_frames_to_exclude)\n",
    "print(\"Frame counts per segments:\", n_frames_dict_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from behavenet import get_user_dir, make_dir_if_not_exists\n",
    "from behavenet.data.utils import get_transforms_paths\n",
    "from behavenet.data.utils import load_labels_like_latents\n",
    "from behavenet.fitting.utils import get_expt_dir\n",
    "from behavenet.fitting.utils import get_session_dir\n",
    "from behavenet.fitting.utils import get_best_model_version\n",
    "from behavenet.fitting.utils import get_lab_example\n",
    "from behavenet.plotting.arhmm_utils import *\n",
    "\n",
    "save_outputs = True  # true to save figures/movies to user's figure directory\n",
    "format = 'png'  # figure format ('png' | 'jpeg' | 'pdf'); movies saved as mp4\n",
    "# model_class = 'arhmm'  # 'arhmm' | 'arhmm-labels'\n",
    "\n",
    "\n",
    "# read the valuse of hparams from meta_tag.csv\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your CSV file\n",
    "# file_path = \"meta_tag.csv\"\n",
    "meta_tag_file_path = '/local1/video-analysis/scratch/706893_2024-05-28_15-15-38/results/xinxin/coupled-baiting_' +bodypart+ '/706893/2024-05-28/arhmm/' +n_ae_latents_str+ '_latents/02_states/stationary/gaussian/arhmm_' +bodypart+ '_3_views/version_0/meta_tags.csv'\n",
    "\n",
    "# Load CSV\n",
    "df_ = pd.read_csv(meta_tag_file_path)\n",
    "\n",
    "# Extract frame rate\n",
    "arhmm_model_experiment_name = df_[df_[\"key\"] == \"experiment_name\"][\"value\"].values[0]\n",
    "n_ae_latents = int(df_[df_[\"key\"] == \"n_ae_latents\"][\"value\"].values[0])\n",
    "experimenter_name = df_[df_[\"key\"] == \"lab\"][\"value\"].values[0]\n",
    "dataset_hparams_experiment_name = df_[df_[\"key\"] == \"expt\"][\"value\"].values[0]\n",
    "frame_rate = float(df_[df_[\"key\"] == \"frame_rate\"][\"value\"].values[0])\n",
    "noise_type = df_[df_[\"key\"] == \"noise_type\"][\"value\"].values[0]\n",
    "n_arhmm_lags = int(df_[df_[\"key\"] == \"n_arhmm_lags\"][\"value\"].values[0])\n",
    "ae_experiment_name = df_[df_[\"key\"] == \"ae_experiment_name\"][\"value\"].values[0]\n",
    "model_class = df_[df_[\"key\"] == \"model_class\"][\"value\"].values[0]\n",
    "arhmm_model_version = df_[df_[\"key\"] == \"version\"][\"value\"].values[0]\n",
    "\n",
    "print(\"arhmm_model_experiment_name:\", arhmm_model_experiment_name)\n",
    "print(\"n_ae_latents:\", n_ae_latents)\n",
    "print(\"experimenter_name:\", experimenter_name)\n",
    "print(\"dataset_hparams_experiment_name:\", dataset_hparams_experiment_name)\n",
    "print(\"frame_rate:\", frame_rate)\n",
    "print(\"noise_type:\", noise_type)\n",
    "print(\"n_arhmm_lags:\", n_arhmm_lags)\n",
    "print(\"ae_experiment_name:\", ae_experiment_name)\n",
    "print(\"model_class:\", model_class)\n",
    "print(\"arhmm_model_version:\", arhmm_model_version)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "device = df_[df_[\"key\"] == \"device\"][\"value\"].values[0]\n",
    "batch_load = df_[df_[\"key\"] == \"batch_load\"][\"value\"].values[0]\n",
    "rng_seed_model = int(df_[df_[\"key\"] == \"rng_seed_model\"][\"value\"].values[0])\n",
    "as_numpy = df_[df_[\"key\"] == \"as_numpy\"][\"value\"].values[0]\n",
    "rng_seed_data = int(df_[df_[\"key\"] == \"rng_seed_data\"][\"value\"].values[0])\n",
    "trial_splits = df_[df_[\"key\"] == \"trial_splits\"][\"value\"].values[0]\n",
    "train_frac = float(df_[df_[\"key\"] == \"train_frac\"][\"value\"].values[0])\n",
    "\n",
    "\n",
    "print(\"device:\", device)\n",
    "print(\"batch_load:\", batch_load)\n",
    "print(\"rng_seed_model:\", rng_seed_model)\n",
    "print(\"as_numpy:\", as_numpy)\n",
    "print(\"rng_seed_data:\", rng_seed_data)\n",
    "print(\"trial_splits:\", trial_splits)\n",
    "print(\"train_frac:\", train_frac)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Extract the version number\n",
    "version = meta_tag_file_path.split(\"version_\")[1].split(\"/\")[0]\n",
    "\n",
    "print(\"version assigned in meta_tag_file_path path: \", version) \n",
    "print(\"'best version' as stored in meta_tag.csv: \", arhmm_model_version)\n",
    "\n",
    "assert version == arhmm_model_version, \"Version mismatch\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set up common hparams\n",
    "hparams = {\n",
    "    'data_dir': get_user_dir('data'),\n",
    "    'save_dir': get_user_dir('save'),\n",
    "    'experiment_name': arhmm_model_experiment_name,\n",
    "    'model_class': model_class,\n",
    "    'model_type': None,\n",
    "    'noise_type': noise_type,\n",
    "    'n_arhmm_lags': n_arhmm_lags,\n",
    "    'transitions': 'stationary',\n",
    "    'ae_experiment_name': ae_experiment_name,\n",
    "    'ae_model_class': 'ae',\n",
    "    'ae_model_type': 'conv',\n",
    "    'ae_version': 'best',\n",
    "    'n_ae_latents': n_ae_latents,\n",
    "    'device': device,\n",
    "    'as_numpy': as_numpy,\n",
    "    'batch_load': batch_load,\n",
    "    'rng_seed_model': rng_seed_model,\n",
    "    'rng_seed_data': rng_seed_data,\n",
    "    'trial_splits': trial_splits,\n",
    "    'train_frac': train_frac,\n",
    "}\n",
    "get_lab_example(hparams, experimenter_name, dataset_hparams_experiment_name)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Construct the path where the CSV was saved.\n",
    "arhmm_latents_path = os.path.join(get_session_dir(hparams)[0], 'dfs', 'arhmm_latents_df.csv')\n",
    "# Read the CSV file into a DataFrame.\n",
    "arhmm_latents_df = pd.read_csv(arhmm_latents_path)\n",
    "\n",
    "# Define file paths\n",
    "df_original_path = os.path.join(get_session_dir(hparams)[0], 'dfs', 'df_original.csv')\n",
    "# Load the CSV files\n",
    "df_original = pd.read_csv(df_original_path)\n",
    "\n",
    "# Get the first Harp_Timestamp from arhmm_latents_df\n",
    "first_harp_timestamp = arhmm_latents_df[\"Harp_Timestamp\"].iloc[0]\n",
    "\n",
    "# Find the closest next trial timestamp in df_original\n",
    "first_analyzed_trial_idx = (df_original[\"start_time\"] > first_harp_timestamp).idxmax()  # Get index of the first trial after first_harp_timestamp in analyzed video\n",
    "first_analyzed_trial_event_timestamp = df_original.loc[first_analyzed_trial_idx, \"start_time\"]\n",
    "first_analyzed_trial_number = df_original.loc[first_analyzed_trial_idx, \"Trial\"]\n",
    "\n",
    "# Print results\n",
    "print(f\"First Harp_Timestamp from arhmm_latents_df: {first_harp_timestamp}\")\n",
    "print(f\"Closest next trial Harp_Timestamp (start_time) from df_original: {first_analyzed_trial_event_timestamp} (Trial {first_analyzed_trial_number})\")\n",
    "print('')\n",
    "\n",
    "# Get the last non-NaN Harp_Timestamp from arhmm_latents_df\n",
    "last_harp_timestamp = arhmm_latents_df[\"Harp_Timestamp\"].dropna().iloc[-1]\n",
    "\n",
    "# # Find the closest prior trial timestamp in df_original\n",
    "# Drop NaNs from the Harp_Timestamp column\n",
    "df_original_clean = df_original.dropna(subset=[\"stop_time\"])\n",
    "\n",
    "# Find the index of the last trial before last_harp_timestamp\n",
    "last_analyzed_trial_idx = df_original_clean[df_original_clean[\"stop_time\"] < last_harp_timestamp].index[-1]\n",
    "last_analyzed_trial_event_timestamp = df_original.loc[last_analyzed_trial_idx, \"stop_time\"]\n",
    "\n",
    "# Get the trial number of the closest prior trial\n",
    "last_analyzed_trial_number = df_original_clean.loc[last_analyzed_trial_idx, \"Trial\"]\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"Last Harp_Timestamp from arhmm_latents_df: {last_harp_timestamp}\")\n",
    "print(f\"Closest prior trial Harp_Timestamp from df_events: {last_analyzed_trial_event_timestamp} (Trial {last_analyzed_trial_number})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ssm\n",
    "from ssm.util import find_permutation\n",
    "from ssm.plots import gradient_cmap, white_to_color_cmap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "color_names = [\n",
    "    \"windows blue\",\n",
    "    \"red\",\n",
    "    \"amber\",\n",
    "    \"faded green\",\n",
    "    \"dusty purple\",\n",
    "    \"orange\",\n",
    "    \"clay\",\n",
    "    \"pink\",\n",
    "    \"greyish\",\n",
    "    \"mint\",\n",
    "    \"light cyan\",\n",
    "    \"steel blue\",\n",
    "    \"forest green\",\n",
    "    \"pastel purple\",\n",
    "    \"salmon\",\n",
    "    \"dark brown\",\n",
    "    \"lavender\",\n",
    "    \"tan\",\n",
    "    \"maroon\",\n",
    "    \"turquoise\",\n",
    "    \"bright green\",\n",
    "    \"gold\",\n",
    "    \"sky blue\"\n",
    "    ]\n",
    "\n",
    "colors_hmm = sns.xkcd_palette(color_names)  #????? bad naming\n",
    "patch_cmap = gradient_cmap(colors_hmm)\n",
    "\n",
    "# Speficy whether or not to save figures\n",
    "save_figures = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Reaction Time vs Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Load the data ===\n",
    "df_original = pd.read_csv(dfs_dir + '/df_original.csv')\n",
    "\n",
    "# === Set your analysis parameters ===\n",
    "# first_analyzed_trial_number = 1  # Replace this with your actual desired trial number\n",
    "# Parameters\n",
    "excluding_initial_late = True  # Set to False to include all data\n",
    "\n",
    "# Filter to rewarded trials only\n",
    "df_original_cp = df_original.copy()\n",
    "\n",
    "# === Step 1: Compute reaction time ===\n",
    "df_original_cp['reaction_time'] = df_original_cp['reward_outcome_time'] - df_original_cp['goCue_start_time']\n",
    "\n",
    "# === Step 2: Filter out trials before the starting point ===\n",
    "# df_filtered = df_original_cp[df_original_cp['Trial'] >= first_analyzed_trial_number].copy()\n",
    "# Apply time filtering if enabled\n",
    "if excluding_initial_late:\n",
    "    df_original_cp = df_original_cp[\n",
    "        (df_original_cp['Trial'] >= first_analyzed_trial_number) &\n",
    "        (df_original_cp['Trial'] <= last_analyzed_trial_number)\n",
    "    ]\n",
    "\n",
    "# === Step 3: Drop rows with missing reaction time ===\n",
    "df_original_cp = df_original_cp.dropna(subset=['reaction_time', 'Trial'])\n",
    "\n",
    "# === Step 4: Plot RT vs. Trial ===\n",
    "plt.figure(figsize=(16, 3))\n",
    "plt.scatter(df_original_cp['Trial'], df_original_cp['reaction_time'], color='blue', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Trial Number')\n",
    "plt.ylabel('Reaction Time (s)')\n",
    "plt.title(f'Reaction Time vs. Trial (starting from Trial {first_analyzed_trial_number})')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# === Parameters ===\n",
    "selected_cluster = 1  # Change this to target the cluster of interest\n",
    "output_dir = f\"./cluster_{selected_cluster}_videos\"\n",
    "# total_num_initial_frames_to_exclude = 1000  # Adjust to your actual value\n",
    "# original_video_path = '/local1/video-analysis/videos/706893_2024-05-28_15-15-38/ear/left_ear.mp4'  # Set to your original video file path\n",
    "original_video_path = '/local1/video-analysis/videos/706893_2024-05-28_15-15-38/face_left/face_left_48fps.mp4'\n",
    "\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === Get Trials from the Selected Cluster ===\n",
    "selected_trials = clustered_df[clustered_df['Cluster'] == selected_cluster]['Trial'].unique()\n",
    "\n",
    "# === Load the video ===\n",
    "cap = cv2.VideoCapture(original_video_path)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Error opening video file\")\n",
    "\n",
    "# === Loop through each trial ===\n",
    "for trial_num in selected_trials:\n",
    "    if trial_num not in trial_data:\n",
    "        print(f\"Trial {trial_num} not in trial_data; skipping.\")\n",
    "        continue\n",
    "    \n",
    "    df_trial = trial_data[trial_num]\n",
    "    frame_indices = df_trial['Timestamp'].astype(int).tolist()\n",
    "    corrected_indices = [i + total_num_initial_frames_to_exclude for i in frame_indices]\n",
    "    \n",
    "    # Set up video writer\n",
    "    output_path = os.path.join(output_dir, f\"cluster{selected_cluster}_trial{trial_num}.mp4\")\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    cap.read()  # Dummy read to initiate\n",
    "\n",
    "    # Get frame properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Read and write selected frames\n",
    "    for frame_idx in corrected_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            writer.write(frame)\n",
    "        else:\n",
    "            print(f\"Warning: Could not read frame {frame_idx} for Trial {trial_num}\")\n",
    "    \n",
    "    writer.release()\n",
    "    print(f\"Saved: {output_path}\")\n",
    "\n",
    "print(fps)\n",
    "# Release the video object\n",
    "cap.release()\n",
    "print(\"All trial clips exported.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "behavenet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
